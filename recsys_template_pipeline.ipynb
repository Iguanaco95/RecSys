{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0267b41f",
   "metadata": {
    "cellId": "ktfx3qyn2fdvuli3xfy71m"
   },
   "source": [
    "### Подготовка данных\n",
    "Для формирования обучающей выборки из датасета с длинной историей выбран подход, при котором в нее с большей вероятностью попадают недавние заказы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e6803",
   "metadata": {
    "cellId": "2larsc97ldz4kvssnzwx56"
   },
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime(orders.created_at.min())\n",
    "end_date = pd.to_datetime(orders.created_at.max())\n",
    "day_delta = (end_date - start_date).days\n",
    "\n",
    "# Для каждой даты устанавливаем свой веc на основании удаленности по времени\n",
    "unique_dates = df_dates.unique()\n",
    "unique_dates['weight'] = unique_dates.apply(lambda x: (pd.to_datetime(x[0]) - start_date).days/day_delta, axis=1)\n",
    "\n",
    "# Join весов к основному датасету\n",
    "orders['created_at'] = df_dates\n",
    "orders = orders.join(unique_dates.set_index('created_at'), how='left', on='created_at')\n",
    "\n",
    "# Нормируем веса\n",
    "weight_sum = orders.weight.sum()\n",
    "orders['prob'] = orders.weight/weight_sum\n",
    "orders_weighted = orders[['id', 'prob']].copy()\n",
    "\n",
    "# Выбираем n_samples случайных сэмплов в соответствии с весами\n",
    "n_samples = 2000000\n",
    "selected_order_ids = np.random.choice(orders_weighted.id.values, n_samples, p=orders_weighted.prob.values, replace=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c67113",
   "metadata": {
    "cellId": "2meolgs7u4gkx0fjn1jj78"
   },
   "source": [
    "Для каждого клиента обучаемся на последней корзине. Для обучения формируем записи двух видов - положительные и отрицательные. Из корзины клиента случайным образом извлекаем товар и помещаем в положительный таргет. Из оставшихся товаров в ассортименте достаем случайный товар и помещаем его в отрицательный таргет. Количество товаров в истории не учитываем, учитываем только их наличие. Для каждого товара создается два бинарных признака: был ли товар в истории и находится ли он в текущей корзине."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a4e5cc",
   "metadata": {
    "cellId": "x6k476c1vfe7q4cblhr2"
   },
   "outputs": [],
   "source": [
    "# Последнюю транзакцию клиента будем использовать как текущую корзину\n",
    "client_basket = df.groupby('client_idx').apply(lambda x: x[x.transaction_datetime == x.transaction_datetime.max()])\n",
    "client_history = df.groupby('client_idx').apply(lambda x: x[x.transaction_datetime != x.transaction_datetime.max()])\n",
    "\n",
    "client_basket.reset_index(drop=True, inplace=True)\n",
    "client_history.reset_index(drop=True, inplace=True)\n",
    "\n",
    "products_list = list(product_id_to_idx.values())\n",
    "\n",
    "def get_negative_sample(products_list, basket):\n",
    "    \"\"\"Получение негативного таргета для корзины клиента. \n",
    "    Негативный таргет - случайный товар из католога за исключением товаров в корзине.\"\"\"\n",
    "    \n",
    "    neg_samples = list(set(products_list)-set(basket))  \n",
    "    return np.random.choice(neg_samples, 1)[0]\n",
    "\n",
    "# Создание отрицательного таргета для каждого клиента\n",
    "neg_target = client_basket.groupby('client_idx').product_idx\\\n",
    "    .apply(lambda x: get_negative_sample(products_list, x.values))\n",
    "\n",
    "# Выберем случайный товар для положительного таргета из корзины\n",
    "target = client_basket.groupby('client_idx').product_idx.apply(lambda x: x.sample(n=1).values[0])\n",
    "target = target.reset_index()\n",
    "\n",
    "# Убираем таргет из корзины\n",
    "without_target = pd.concat([client_basket[['client_idx', 'product_idx']], target, target]).drop_duplicates(keep=False)\n",
    "\n",
    "# Не учитываем количество товаров в истории\n",
    "client_history = client_history[['client_idx', 'product_idx']].drop_duplicates() \n",
    "\n",
    "# Делаем матричное представление для пар клиент-товар_в_истории\n",
    "client_history['value'] = [True]*client_history.shape[0]\n",
    "client_history = client_history.pivot(index='client_idx', columns='product_idx')['value'].fillna(False).add_prefix('h_')\n",
    "\n",
    "# Делаем матричное представление для пар клиент-товар_в_корзине\n",
    "without_target['value'] = [True]*without_target.shape[0]\n",
    "without_target = without_target.pivot(index='client_idx', columns='product_idx')['value'].fillna(False).add_prefix('b_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a5bf0",
   "metadata": {
    "cellId": "9vfz8t2tj4qfm6qugsya97"
   },
   "source": [
    "Далее добавляем атрибуты клиентов в основной датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee97b5f",
   "metadata": {
    "cellId": "1r00mztvtswqelwh3lmsf"
   },
   "outputs": [],
   "source": [
    "client_full_df['age'] = clients['age']\n",
    "client_full_df['gender'] = clients['gender']\n",
    "\n",
    "client_full_df.gender.fillna('U', inplace=True)\n",
    "client_full_df.age.fillna(mode, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb39a6",
   "metadata": {
    "cellId": "eoifn7o3jwh0hskicu75o"
   },
   "source": [
    "#### Расчет RFM (recency, frequency, monetary) агрегатов:\n",
    "Для учета персональной истории покупок рассчитываем и добавляем в качестве придикторов RFM-агрегаты. \n",
    "Рассчитываемые предикторы показывают\n",
    "1. Количество дней после покупки товара\n",
    "2. Количество покупок товара в истории\n",
    "3. Относительные траты на товар\n",
    "4. Сумму трат на товар за год\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb854002",
   "metadata": {
    "cellId": "wm8h39arsbmjgzehyv5rt"
   },
   "outputs": [],
   "source": [
    "h_columns = ['last_purch_days_' + str(x) for x in range(len(products_list))]\n",
    "time_last_purch = np.full(len(products_list), np.inf)\n",
    "\n",
    "# Количество дней после покупки товара\n",
    "def get_last_purch_days(x):\n",
    "\n",
    "    hist = df[(df.client_idx == x.client_idx) & (df.transaction_datetime < x.transaction_datetime)]\n",
    "\n",
    "    hist = hist[['product_idx', 'transaction_datetime']]\n",
    "    hist.drop_duplicates(keep='last', inplace=True)\n",
    "    hist['cur_tdt'] = x.transaction_datetime\n",
    "    hist.set_index('product_idx', inplace=True)\n",
    "    hist['time_last_purch'] = (hist['cur_tdt'] - hist['transaction_datetime']).apply(lambda x: x.days)\n",
    "    time_last_purch_ = time_last_purch.copy()\n",
    "    time_last_purch_[hist.index.values] = hist['time_last_purch'].values\n",
    "   \n",
    "    return pd.Series(time_last_purch_, index=h_columns)\n",
    "\n",
    "old_trans_last_purch = old_trans_.apply(get_last_purch_days, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "h_columns = ['h_' + str(x) for x in range(len(products_list))]\n",
    "zeros_hist = np.zeros(len(products_list)).astype(int)\n",
    "\n",
    "# Количество покупок товара в истории\n",
    "def get_history_products(x):\n",
    "\n",
    "    hist = df[(df.client_idx == x.client_idx) & (df.transaction_datetime < x.transaction_datetime)]\n",
    "\n",
    "    vc = hist.product_idx.value_counts()\n",
    "\n",
    "    exp_hist = zeros_hist.copy()\n",
    "    exp_hist[vc.index.values] = vc.values\n",
    "\n",
    "    return pd.Series(exp_hist, index=h_columns)\n",
    "\n",
    "old_trans_frequency = old_trans_.apply(get_history_products, axis=1)\n",
    "old_trans_frequency.columns = ['hist_prod_freq_' + str(x) for x in range(len(products_list))]\n",
    "old_trans_frequency = old_trans_frequency.div(old_trans_frequency.sum(axis=1), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "h_columns = ['hist_purch_sum_' + str(x) for x in range(len(products_list))]\n",
    "zeros_sum = np.zeros(len(products_list))\n",
    "\n",
    "# Относительные траты на товар\n",
    "def get_purch_sum(x):\n",
    "\n",
    "    hist = df[(df.client_idx == x.client_idx) & (df.transaction_datetime < x.transaction_datetime)]\n",
    "\n",
    "    hist = hist[['product_idx', 'trn_sum_from_iss']]\n",
    "    hist = hist.groupby('product_idx').sum()\n",
    "    \n",
    "    zeros_sum_ = zeros_sum.copy()\n",
    "    zeros_sum_[hist.index.values] = hist['trn_sum_from_iss'].values\n",
    "   \n",
    "    return pd.Series(zeros_sum_, index=h_columns)\n",
    "\n",
    "old_trans_purch_summ = old_trans_.apply(get_purch_sum, axis=1)\n",
    "old_trans_purch_relative_summ = old_trans_purch_summ.div(old_trans_purch_summ.sum(axis=1), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "h_columns = ['hist_purch_period_sum_' + str(x) for x in range(len(products_list))]\n",
    "zeros_sum = np.zeros(len(products_list))\n",
    "\n",
    "# Сумма трат на товар за год\n",
    "def get_purch_period_sum(x):\n",
    "\n",
    "    hist = df[(df.client_idx == x.client_idx) & (df.transaction_datetime < x.transaction_datetime)].copy()\n",
    "    \n",
    "    hist['diff_days'] = hist.apply(lambda y: (x.transaction_datetime - y.transaction_datetime).days, axis=1)\n",
    "    hist = hist[hist['diff_days'] < 366]\n",
    "\n",
    "    hist = hist[['product_idx', 'trn_sum_from_iss']]\n",
    "    hist = hist.groupby('product_idx').sum()\n",
    "    \n",
    "    zeros_sum_ = zeros_sum.copy()\n",
    "    zeros_sum_[hist.index.values] = hist['trn_sum_from_iss'].values\n",
    "   \n",
    "    return pd.Series(zeros_sum_, index=h_columns)\n",
    "\n",
    "old_trans_purch_period_summ = old_trans_.apply(get_purch_period_sum, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d055a0f",
   "metadata": {
    "cellId": "ydd5ax0668gjb3ajrvioi"
   },
   "source": [
    "### Обучение\n",
    "Алгоритм градиентного бустинга обучается на следующем наборе предикторов:\n",
    "\n",
    "- Вектор наличия товара в истории (длина вектора - кол-во всех товаров в ассортименте)\n",
    "- Вектор наличия товара в корзине\n",
    "- Временные предикторы\n",
    "- Атрибуты клиента (пол, возраст)\n",
    "- Атрибуты товара в таргете\n",
    "- Скор от SVD\n",
    "- RFM агрегаты\n",
    "\n",
    "Скор от SVD рассчитывается как косинусная близость веркторного представления товара в таргете и суммы представлений товаров в корзине.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536cac3",
   "metadata": {
    "cellId": "eeni29gkfjocy1l4o2yi2"
   },
   "outputs": [],
   "source": [
    "# Для SVD используем все транзакции клиентов, которые (клиенты) участвуют в обучении XGB\n",
    "train_svd = train_transactions[train_transactions.client_idx.isin(train.index.values)]\n",
    "\n",
    "# Формирование разряженной матрицы товар-чек\n",
    "rows = train_svd.transaction_idx.astype(int)\n",
    "cols = train_svd.product_idx.astype(int)\n",
    "sp_train = scipy.sparse.coo_matrix((np.ones_like(rows), (rows, cols)))\n",
    "sp_train = sp_train.toarray() / sp_train.sum(axis=0)\n",
    "svd_model = RecommenderSVDKNN(emb_dim=500)\n",
    "svd_model.fit(sp_train.T)\n",
    "\n",
    "# Подсчет скора от SVD\n",
    "train['knn_score'] = train.apply(lambda x: cosine_similarity(svd_model.X_train_svd[x.target_product].reshape(1, -1), svd_model.X_train_svd[x[bask].values.astype(bool)].sum(0).reshape(1, -1))[0][0], axis=1)\n",
    "\n",
    "# Перемешиваем выборку для обучения\n",
    "train = train.sample(frac=1)\n",
    "\n",
    "print(\"start train xgb\")\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:logistic\", \n",
    "                             #tree_method='gpu_hist', \n",
    "                             #gpu_id=0, \n",
    "                             random_state=42, \n",
    "                             n_jobs=-1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=300)\n",
    "\n",
    "xgb_model.fit(train.drop('target', axis=1).values, train.target.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b1bfa4",
   "metadata": {
    "cellId": "wp0latkfebdtj1oelaaa"
   },
   "source": [
    "### Валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab34b2",
   "metadata": {
    "cellId": "xc80l8ivtts2cbgogkz6ty"
   },
   "outputs": [],
   "source": [
    "def gpu_validate(test_slice):\n",
    "    \"\"\"Функция валидации тестового датасета. Расчет косинусной близости реализован на pytorch.\n",
    "    Итерация происходит по каждому клиенту в тестовом датасете. \n",
    "    В ходе итерации собирается батч вида клиент-товар, где атрибуты клиента фиксированны, \n",
    "    а изменяется только товар в таргете и его атрибуты.\"\"\"\n",
    "    \n",
    "    target_input = test_slice['target_product'].values\n",
    "    test_input = []\n",
    "\n",
    "    for index, user in test_slice.drop(products.columns, axis=1).iterrows():\n",
    "\n",
    "        # Текущая корзина клиента\n",
    "        basket = user['b_0':'b_199']\n",
    "\n",
    "        # Собираем батч клиент-товар\n",
    "        user_df = pd.concat([user] * PRODUCTS_COUNT, axis=1).transpose().drop('target', axis=1)\n",
    "        user_df = pd.concat([user_df.reset_index(drop=True), products], axis=1)\n",
    "        \n",
    "        # Считаем скор SVD на gpu, если доступна видеокарта\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        a = torch.from_numpy(svd_model.X_train_svd).to(device)\n",
    "        b = torch.from_numpy(svd_model.X_train_svd[basket.values.astype(bool)].sum(0).reshape(1, -1)).to(device)\n",
    "        a_norm = a / a.norm(dim=1)[:, None]\n",
    "        b_norm = b / b.norm(dim=1)[:, None]\n",
    "        res = torch.mm(a_norm, b_norm.transpose(0,1)).cpu().numpy()\n",
    "\n",
    "        user_df['knn_score'] = res\n",
    "        user_df = user_df[dataset_columns]\n",
    "\n",
    "        y_pred = xgb_model.predict(user_df.values)\n",
    "\n",
    "        # Ранжируем товары, исключаем товары из корзины\n",
    "        y_pred[basket.values.astype(bool)] = -np.inf\n",
    "        preds = np.argpartition(y_pred, -3, axis=0)[-3:]\n",
    "        test_input.append(list(preds))\n",
    "\n",
    "    return (target_input, test_input)\n",
    "\n",
    "\n",
    "# На тесте используем только положительный таргет\n",
    "test = test[test.target == 1]\n",
    "# Сохраняем товары из таргета для расчета метрики\n",
    "target_input = test['target_product'].values\n",
    "# Массив для прогнозирования\n",
    "test_input = []\n",
    "# Сохраняем порядок предикторов для XGB\n",
    "dataset_columns = train.drop('target', axis=1).columns  \n",
    "\n",
    "result = gpu_validate(test)\n",
    "\n",
    "score = mean_precision(result[0], result[1])\n",
    "print('Mean Precision:', score)\n",
    "scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9289a401",
   "metadata": {
    "cellId": "aj6bpnu0ob5kxyt2zvjxy"
   },
   "source": [
    "### Формирование рекомендаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d89c0600",
   "metadata": {
    "cellId": "makjy6pb6zipfdqqjl6qzp"
   },
   "outputs": [],
   "source": [
    "class XGBRecommender():\n",
    "    \"\"\"Класс-обертка для градиентного бустинга.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Инициализация класса. Загрузка состояний моделей и датасетов.\"\"\"\n",
    "             \n",
    "        self.svd_model = self.load_model('svd_model')\n",
    "        self.xgb_model = self.load_model('xgb_model')\n",
    "        \n",
    "        self.dataset = pd.read_csv(ft.reduce(os.path.join, ['..', 'data', 'processed', 'rfm_client_hist_context_target_product.csv']), index_col=0)\n",
    "        self.dataset = self.dataset[self.dataset.target == 1]\n",
    "        self.dataset.drop(['is_new_client'], axis=1, inplace=True)\n",
    "        \n",
    "        self.products = pd.read_csv(ft.reduce(os.path.join, ['..', 'data', 'processed', 'products.csv']), index_col=0)\n",
    "        self.prod_num = self.products.shape[0]\n",
    "        self.products = self.products.sort_index()\n",
    "        self.products['target_product'] = range(self.prod_num) \n",
    "   \n",
    "        \n",
    "    def load_model(self, name):\n",
    "        \n",
    "        path = ft.reduce(os.path.join, ['..', 'models','pkl' ,name+'.pkl'])\n",
    "        with open(path, 'rb') as handle:\n",
    "            return pickle.load(handle)         \n",
    "    \n",
    "    def predict_sample(self, user_id, basket, store_idx, time, slot_num):\n",
    "        \"\"\"Функция возвращает рекомендованные к покупке товары и их рейтинг.\n",
    "        Рекомендации осуществляются на основе id клиента и его текущей корзины. \"\"\"\n",
    "        \n",
    "        basket_header = ['b_' + str(x) for x in range(self.prod_num)]\n",
    "        history_header = ['h_' + str(x) for x in range(self.prod_num)]\n",
    "\n",
    "        # Формируем историю покупок клиента\n",
    "        user = self.dataset.loc[user_id].copy()\n",
    "        user.loc[history_header] = user.loc[basket_header].values | user.loc[history_header].values\n",
    "        user.loc['h_'+str(user.target_product)] = True\n",
    "\n",
    "        # Приводим список товаров в корзине к вектору-индикатору наличия товара в корзине\n",
    "        basket_ = np.full((self.prod_num), False, dtype=bool)\n",
    "        basket_[basket] = True\n",
    "        user.loc[basket_header] = basket_\n",
    "        user.drop(self.products.columns, inplace=True)\n",
    "        \n",
    "        # Собираем контекст\n",
    "        time = pd.to_datetime(time)\n",
    "        user['store_idx'] = store_idx\n",
    "        user['product_count'] = np.sum(basket_)\n",
    "        user['hour'] = time.hour\n",
    "        user['dayofweek'] = time.dayofweek\n",
    "        user['day'] = time.day\n",
    "        user['month'] = time.month\n",
    "        user['year'] = time.year\n",
    "        \n",
    "        # Готовим датасет вида клиент(контекст)-товары для скоринга товаров\n",
    "        user_df = pd.concat([user] * self.prod_num, axis=1).transpose().drop('target', axis=1)\n",
    "        user_df = pd.concat([user_df.reset_index(drop=True), self.products], axis=1)\n",
    "        \n",
    "        user_df = user_df[self.dataset.drop('target', axis=1).columns]\n",
    "        \n",
    "        # Рассчитываем скор SVD_KNN\n",
    "        user_df['knn_score'] = cosine_similarity(self.svd_model.X_train_svd, self.svd_model.X_train_svd[basket].sum(0).reshape(1, -1))\n",
    "\n",
    "        y_pred = self.xgb_model.predict(user_df.values)\n",
    "        \n",
    "        # Ранжируем, отбираем топ        \n",
    "        y_pred[basket] = -np.inf\n",
    "\n",
    "        recs = np.argpartition(y_pred, -slot_num, axis=0)[-slot_num:]\n",
    "        recs = list(recs[np.argsort(y_pred[recs])])\n",
    "        recs.reverse()\n",
    "        \n",
    "        return recs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54debd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "notebookId": "9cbb945c-7d5a-4dd3-a026-7fdb2476a2a8",
  "notebookPath": "recommendation-system/notebooks/product-recommender.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
